<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>EDA | BankIntel</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700&display=swap" rel="stylesheet">
  <style>
    body { font-family: 'Poppins', sans-serif; }
    html { scroll-behavior: smooth; }
  </style>
</head>
<body class="bg-gray-950 text-white">

<!-- Navbar -->
<nav class="bg-gray-900 p-4 sticky top-0 z-50 shadow-lg">
  <div class="max-w-7xl mx-auto flex justify-between items-center">
    <a href="index.html#home" class="text-xl font-bold flex items-center gap-2 hover:text-blue-400">
      üè¶ BankIntel: Insights for Smarter Banking
    </a>
    <ul class="flex space-x-6 text-sm relative">
      <li><a href="index.html#intro" class="hover:text-blue-400">Introduction</a></li>
      <li><a href="eda.html" class="hover:text-blue-400">EDA</a></li>

      <!-- Models Dropdown -->
      <li class="relative group">
        <button class="hover:text-blue-400 flex items-center gap-1 font-semibold">
          Models Implemented <svg class="w-3 h-3 mt-[1px]" fill="currentColor" viewBox="0 0 20 20"><path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.585l3.71-4.354a.75.75 0 111.14.976l-4.25 5a.75.75 0 01-1.14 0l-4.25-5a.75.75 0 01.02-1.06z" clip-rule="evenodd" /></svg>
        </button>

        <!-- Dropdown -->
        <div class="absolute left-0 mt-2 w-56 bg-white text-gray-800 rounded-lg shadow-lg hidden group-hover:block z-50">
          <div class="px-4 py-3 border-b">
            <p class="text-sm font-medium text-gray-600 mb-1">Unsupervised Learning</p>
            <ul class="space-y-1 pl-2">
              <li><a href="pca.html" class="block hover:text-blue-500">PCA</a></li>
              <li><a href="clustering.html" class="block hover:text-blue-500">Clustering</a></li>
              <li><a href="arm.html" class="block hover:text-blue-500">ARM</a></li>
            </ul>
          </div>
          <div class="px-4 py-3">
            <p class="text-sm font-medium text-gray-600 mb-1">Supervised Learning</p>
            <ul class="space-y-1 pl-2">
              <li><a href="naive-bayes.html" class="block hover:text-blue-500">Na√Øve Bayes</a></li>
              <li><a href="dt.html" class="block hover:text-blue-500">DT</a></li>
              <li><a href="regression.html" class="block hover:text-blue-500">Regression</a></li>
              <li><a href="svm.html" class="block hover:text-blue-500">SVM</a></li>
              <li><a href="ensemble.html" class="block hover:text-blue-500">Ensemble</a></li>
              <li><a href="train-test.html" class="block hover:text-blue-500">Train-Test Split</a></li>
            </ul>
          </div>
        </div>
      </li>

      <li><a href="conclusion.html" class="hover:text-blue-400">Conclusion</a></li>
      <li><a href="contact.html" class="hover:text-blue-400">Contact</a></li>
    </ul>
  </div>
</nav>

<!-- Hero -->
<section class="relative min-h-[50vh] flex flex-col justify-center items-center text-center px-6 py-20 bg-cover bg-center bg-no-repeat" style="background-image: url('assets/analytics.jpg');">
  <div class="absolute inset-0 bg-gradient-to-b from-black/80 via-black/60 to-black/90 z-0"></div>
  <div class="relative z-10 bg-black bg-opacity-60 p-6 md:p-10 rounded-lg shadow-lg shadow-black/30">
    <h1 class="text-3xl md:text-5xl font-bold text-white flex items-center gap-3">
      <span>Principal Component Analysis (PCA)</span>
    </h1>
  </div>
</section>

<!-- PCA Overview Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">
  <h2 class="text-3xl font-bold text-white flex items-center gap-2 mb-4">
    Overview
  </h2>
  <hr class="border-gray-700 mb-6" />
  <p class="text-gray-300 text-lg leading-relaxed text-justify mb-4">
    Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a smaller set of uncorrelated features, called principal components, while preserving as much variance as possible. It works by identifying directions (components) that capture the most variation in the dataset, ranking them based on their importance. The first few components usually contain the majority of the information, allowing for data visualization, noise reduction, and improved model performance.
  </p>
  <p class="text-gray-300 text-lg leading-relaxed text-justify">
    In this project, PCA was applied to transform the dataset into a lower-dimensional space, making it easier to analyze while preserving 95% of the variance. The process involved standardizing the data, computing principal components, and selecting the optimal number of components for effective modeling.
  </p>
</div>

<!-- PCA Dimensionality Reduction Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">

<!-- Image -->
  <div class="flex justify-center mb-6">
    <img src="assets/pca_red.png" alt="PCA Explanation" class="w-full max-w-xl rounded-lg shadow-md">
  </div>

  <!-- Description -->
  <p class="text-gray-300 text-lg leading-relaxed text-justify">
    The image above shows how PCA (Principal Component Analysis) helps reduce high-dimensional data into a simpler, lower-dimensional form.
    On the left, data with three variables is shown in 3D. PCA finds new directions, called <span class="text-red-400 font-medium">principal components</span>, that capture the most variation in the data.
    The right side shows how the data is projected onto just two components (<span class="text-red-400 font-medium">PC1</span> and <span class="text-blue-400 font-medium">PC2</span>),
    keeping most of the important information while making it easier to work with and visualize.
    <br /><br />
    Although PCA is a powerful technique, it can make transformed features harder to interpret as they no longer align with original variable meanings.
    However, it remains an effective tool for improving model performance, speeding up computations, and gaining insights through visualization.
  </p>
</div>

<!-- PCA: Principal Components Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">

  <!-- Image -->
   <div class="flex justify-center mb-6">
    <img src="assets/pca3.png" alt="3D PCA projection"
       class="w-[300px] md:w-[400px] rounded-lg shadow-md" />
    </div>

  <!-- Description -->
  <p class="text-gray-300 text-lg leading-relaxed text-justify">
    This image visualizes the result of applying PCA on a dataset with three dimensions. The yellow dots represent data points in 3D space, and the gray plane shows the lower-dimensional space formed by the first two principal components, <span class="text-red-400 font-medium">PC1</span> and <span class="text-blue-400 font-medium">PC2</span>. PCA projects the data onto this plane because it captures the majority of the data‚Äôs variance. <span class="text-yellow-400 font-medium">PC3</span>, which contributes less to the overall variance, is not used in the projection. This helps simplify the data while still preserving the most important patterns for analysis or modeling.
  </p>
</div>

<!-- PCA: Principal Components Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">

  <!-- Description -->
    <p class="text-gray-300 text-lg leading-relaxed text-justify mt-4">
    <span class="text-white font-semibold">Orthogonality</span> is a key concept in PCA because it ensures that each principal component captures unique, non-overlapping information from the dataset. Since the components are orthogonal, they are uncorrelated, meaning no redundancy exists between them. This helps in breaking down complex data into clean, independent directions of variation, making the analysis more interpretable and reducing noise. Orthogonality also ensures that each component adds distinct value to the transformed dataset, improving both visualization and the performance of downstream machine learning models.
  </p>
</div>

<div class="flex justify-center mt-8">
  <a href="https://github.com/moukthika-gunapaneedu/Bank-Marketing-Campaign-Analyzer/blob/main/Code/Moukthika_Gunapaneedu_PCA.ipynb" 
     target="_blank"
     class="px-8 py-3 bg-blue-600 text-white text-lg font-semibold rounded-lg shadow-lg hover:bg-blue-500 hover:scale-105 transition transform duration-200 ease-in-out">
    üíª CODE FOR IMPLEMENTATION OF PCA
  </a>
</div>

<div class="max-w-4xl mx-auto my-10">
  <figure class="bg-gray-900 border border-gray-700 rounded-xl shadow-lg p-4">
    <img src="assets/clean_data.png" alt="Clean dataset preview" class="rounded-md w-full object-contain">
    <figcaption class="mt-4 text-center text-gray-400 text-sm"><em>DATASET BEFORE PREPARING FOR PCA</em></figcaption>
  </figure>
</div>

<div class="flex justify-center mt-8">
  <a href="https://github.com/moukthika-gunapaneedu/Bank-Marketing-Campaign-Analyzer/blob/main/Data/bank_cleaned_data.csv" 
     target="_blank"
     class="px-8 py-3 bg-blue-600 text-white text-lg font-semibold rounded-lg shadow-lg hover:bg-blue-500 hover:scale-105 transition transform duration-200 ease-in-out">
    üóÉÔ∏è CLICK HERE TO VIEW THE DATASET BEFORE PREPARING FOR PCA
  </a>
</div>

<!-- Data Selection and Preprocessing Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">
  <h2 class="text-3xl font-bold text-white flex items-center gap-2 mb-4">
    Data Selection and Preprocessing
  </h2>
  <hr class="border-gray-700 mb-6" />

  <p class="text-gray-300 text-lg leading-relaxed text-justify">
    To ensure that all numerical features contribute equally to the PCA process, the dataset was standardized using 
    <span class="text-white font-semibold">StandardScaler</span> from <span class="text-white font-semibold">sklearn</span>. 
    Standardization transforms the data so that each feature has:
  </p>

  <ul class="text-gray-300 text-lg list-disc list-inside ml-4 my-4">
    <li><span class="text-white font-medium">Mean = 0</span></li>
    <li><span class="text-white font-medium">Standard Deviation = 1</span></li>
  </ul>

  <p class="text-gray-300 text-lg leading-relaxed text-justify">
    This prevents features with larger numerical ranges (e.g., <span class="text-white">balance</span> or 
    <span class="text-white">duration</span>) from dominating those with smaller ranges. Only numerical columns 
    were selected for PCA, and the standardized dataset was prepared for dimensionality reduction.
  </p>
</div>

<div class="max-w-4xl mx-auto my-10">
  <figure class="bg-gray-900 border border-gray-700 rounded-xl shadow-lg p-4">
    <img src="assets/pca_data.png" alt="Clean dataset preview" class="rounded-md w-full object-contain">
    <figcaption class="mt-4 text-center text-gray-400 text-sm"><em>DATASET AFTER PREPARING FOR PCA</em></figcaption>
  </figure>
</div>

<div class="flex justify-center mt-8">
  <a href="https://github.com/moukthika-gunapaneedu/Bank-Marketing-Campaign-Analyzer/blob/main/Data/PCA/bank_data_scaled_for_pca.csv" 
     target="_blank"
     class="px-8 py-3 bg-blue-600 text-white text-lg font-semibold rounded-lg shadow-lg hover:bg-blue-500 hover:scale-105 transition transform duration-200 ease-in-out">
    üóÉÔ∏è CLICK HERE TO VIEW THE DATASET AFTER PREPARING FOR PCA
  </a>
</div>

<!-- Why StandardScaler Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">
  <h2 class="text-3xl font-bold text-white flex items-center gap-2 mb-4">
     Why StandardScaler?
  </h2>
  <hr class="border-gray-700 mb-6" />

  <p class="text-gray-300 text-lg leading-relaxed text-justify">
    <span class="text-white font-semibold">StandardScaler</span> is used to normalize the dataset before applying PCA 
    to ensure that all features contribute equally. Without standardization, features with larger numerical ranges 
    (e.g., <span class="text-white">balance</span>, <span class="text-white">duration</span>) would dominate those 
    with smaller ranges, distorting the principal components.
  </p>

  <p class="text-gray-300 text-lg leading-relaxed text-justify mt-4">
    <span class="text-white font-semibold">StandardScaler</span> transforms the data by centering it around zero 
    (<span class="text-white">mean = 0</span>) and scaling it to unit variance 
    (<span class="text-white">standard deviation = 1</span>). This step is crucial for PCA, 
    as it relies on variance to determine the importance of each principal component.
  </p>
</div>

<!-- Applying PCA with 2 and 3 Components Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">
  <h2 class="text-3xl font-bold text-white flex items-center gap-2 mb-4">
   Applying PCA with 2 and 3 Components
  </h2>
  <hr class="border-gray-700 mb-6" />

  <p class="text-gray-300 text-lg leading-relaxed text-justify mb-4">
    To visualize and analyze the dataset in a lower-dimensional space, PCA was performed with <span class="text-white font-semibold">2</span> and <span class="text-white font-semibold">3 components</span>:
  </p>

  <p class="text-gray-300 text-lg leading-relaxed text-justify mb-2">
    <span class="text-white font-semibold">Variance Retention in PCA</span>
  </p>

  <ul class="list-disc list-inside text-gray-300 text-lg leading-relaxed pl-4">
    <li><span class="text-white font-semibold">PCA with 2 components</span> retains <span class="text-white font-semibold">38.06%</span> of the variance, meaning that this lower-dimensional representation captures only part of the dataset‚Äôs structure, losing a significant amount of information.</li>
    <li class="mt-2"><span class="text-white font-semibold">PCA with 3 components</span> retains <span class="text-white font-semibold">53.73%</span> of the variance, providing a more detailed representation but still missing nearly half of the original variance.</li>
  </ul>
</div>

<!-- PCA Code with Output Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">
  <h2 class="text-3xl font-bold text-white flex items-center gap-2 mb-4">
  PCA Code and Output
  </h2>
  <hr class="border-gray-700 mb-6" />

  <!-- Code Block -->
  <pre class="bg-gray-800 text-green-200 text-sm rounded-lg overflow-x-auto p-4 mb-6">
<code># perform PCA with 2 and 3 components
pca_2 = PCA(n_components=2)
pca_3 = PCA(n_components=3)

df_pca_2 = pca_2.fit_transform(df_scaled)
df_pca_3 = pca_3.fit_transform(df_scaled)

# explained variance ratio
explained_variance_2 = np.sum(pca_2.explained_variance_ratio_) * 100
explained_variance_3 = np.sum(pca_3.explained_variance_ratio_) * 100

# explained variance percentages
explained_variance_2, explained_variance_3
  </code></pre>

<!-- Output Box -->
<div class="bg-gray-800 text-white text-sm font-mono px-4 py-3 rounded-lg shadow-inner mb-6">
  <span class="text-green-400">Out:</span> (38.0615%, 53.7332%)
</div>

<!-- Explanatory Text with top margin -->
<p class="text-gray-300 text-lg leading-relaxed text-justify mt-6">
  These values indicate that while PCA reduces complexity, using only 2 or 3 components is insufficient to fully preserve the dataset's structure. The corresponding cumulative variance plots shown below visually confirm the variance retained at each component level.
</p>

</div>

<!-- PCA 2-Component Scatter Plot Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">
  <h2 class="text-2xl font-bold text-white flex items-center gap-2 mb-2">
    PCA with 2 Components
  </h2>
  <hr class="border-gray-700 mb-6" />

  <div class="flex justify-center mb-4">
    <img src="assets/pca_2comp.png" alt="PCA with 2 components" class="w-[500px] rounded-lg shadow-md" />
  </div>

  <p class="text-gray-300 text-base leading-relaxed text-center max-w-3xl mx-auto">
    This scatter plot represents the dataset after applying PCA with 2 components, capturing <strong class="text-white">38.06%</strong> of the total variance. Most data points are clustered near the origin, with a few outliers. While this reduction helps in visualization, it does not retain enough variance for accurate modeling.
  </p>
</div>

<!-- PCA 3 Components Visualization Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-12">
  <h2 class="text-2xl font-bold text-white flex items-center gap-2 mb-4">
    PCA with 3 Components
  </h2>
  <hr class="border-gray-700 mb-6" />

  <div class="flex justify-center">
    <img src="assets/pca_3comp.png" alt="PCA 3 Components" class="w-[500px] rounded-lg shadow-md" />
  </div>

  <p class="text-gray-300 text-center text-base mt-4 max-w-3xl mx-auto">
    The 3D scatter plot provides a more informative representation by adding a third principal component, retaining <strong class="text-white">53.73%</strong> of the variance. While this visualization offers better data separation than the 2D version, it still does not preserve the majority of the dataset‚Äôs information.
  </p>
</div>

<!-- PCA Optimal Components Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-12">
  <h2 class="text-2xl font-bold text-white flex items-center gap-2 mb-4">
    Determining the Optimal Number of PCA Components
  </h2>
  <hr class="border-gray-700 mb-6" />

  <p class="text-gray-300 text-lg leading-relaxed text-justify mt-6">
    The code below performs PCA on the entire dataset to determine the number of principal components needed to retain at least
    <strong class="text-white">95% of the variance</strong>. By computing the cumulative explained variance, it was found that
    <strong class="text-white">7 principal components</strong> are required to achieve this threshold. Additionally, the top three eigenvalues were extracted
    (<strong class="text-white">1.5093</strong>, <strong class="text-white">1.1550</strong>, and <strong class="text-white">1.0970</strong>), indicating the relative importance
    of the first three principal components in capturing variance. This confirms that while dimensionality reduction is effective, retaining too few components
    would lead to significant information loss.
  </p>

  <!-- Code Block -->
  <pre class="bg-gray-800 text-green-200 text-sm rounded-md p-4 overflow-x-auto mt-6 mb-4">
<span class="text-gray-400"># perform PCA with all components to find the number needed for 95% variance</span>
pca_full = PCA()
pca_full.fit(df_scaled)

<span class="text-gray-400"># cumulative explained variance</span>
cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)

<span class="text-gray-400"># find the number of components needed for 95% variance</span>
num_components_95 = np.where(cumulative_variance >= 0.95)[0][0] + 1

<span class="text-gray-400"># extract the top 3 eigenvalues</span>
top_3_eigenvalues = pca_full.explained_variance_[:3]

num_components_95, top_3_eigenvalues
  </pre>

  <!-- Output -->
  <div class="bg-gray-800 rounded-md px-4 py-3 text-green-300 font-mono text-sm mb-6">
    <span class="text-white font-semibold">Out:</span> <span>(7, array([1.5093, 1.1550, 1.0970]))</span>
  </div>
</div>

<!-- Cumulative Variance Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-12">
  <h2 class="text-2xl font-bold text-white flex items-center gap-2 mb-4">
    Cumulative Variance Explained by PCA Components
  </h2>
  <hr class="border-gray-700 mb-6" />
  
  <p class="text-gray-300 text-lg leading-relaxed text-justify mb-8">
    The graph illustrates how the cumulative variance increases as more principal components are added. 
    The red dashed line marks the 95% variance threshold, which is reached at 7 principal components. 
    This means that reducing the dataset to 7 dimensions preserves most of the original information while significantly lowering complexity. 
    The plot confirms that using fewer components (e.g., 2 or 3) retains only a fraction of the variance, leading to potential information loss.
  </p>

  <div class="flex justify-center mb-4">
    <img src="assets/cumulative_variance_pca.png" alt="Cumulative variance explained" class="w-[500px] rounded-lg shadow-md">
  </div>
</div>

<!-- PCA Variance Bar Plot Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-12">
  <h2 class="text-2xl font-bold text-white flex items-center gap-2 mb-4">
    Illustration of the 7 Principal Components
  </h2>
  <hr class="border-gray-700 mb-6" />

  <p class="text-gray-300 text-lg leading-relaxed text-justify mb-6">
    This bar plot shows the variance explained by each of the first 7 principal components. The first principal component contributes the most variance (21.56%), followed by the second (16.50%) and third (15.67%). The variance contribution gradually decreases with each additional component.
  </p>

  <div class="flex justify-center mb-6">
    <img src="assets/variance_pca.png" alt="Variance explained by PCA components" class="w-[500px] rounded-lg shadow-md">
  </div>

  <p class="text-gray-300 text-lg leading-relaxed text-justify">
    The PCA-transformed dataset represents the original features in a reduced-dimensional space, capturing 95% of the variance with 7 principal components (PC1 to PC7). Each principal component is a linear combination of the original features, optimized to retain the most critical information while reducing complexity. This transformation enables efficient modeling and visualization while minimizing information loss.
  </p>
</div>

<!-- Variance Explained Section -->
<section class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">
  <h2 class="text-2xl font-bold text-white flex items-center gap-2 mb-4">Variance Explained by Each Component</h2>
  <hr class="border-gray-700 mb-6" />

  <p class="text-gray-300 text-lg leading-relaxed mb-6">
    Each principal component captures a portion of the dataset‚Äôs total variance:
  </p>

  <ul class="text-gray-300 space-y-3 pl-5 list-disc text-lg leading-relaxed">
    <li><strong class="text-white">PC1</strong> explains <span class="text-blue-400">21.56%</span> of the total variance. It captures the most significant pattern in the data.</li>
    <li><strong class="text-white">PC2</strong> explains <span class="text-blue-400">16.50%</span>, representing the second strongest direction of variation, uncorrelated with PC1.</li>
    <li><strong class="text-white">PC3</strong> accounts for <span class="text-blue-400">15.67%</span> of the variance, capturing additional patterns not seen in PC1 or PC2.</li>
    <li><strong class="text-white">PC4</strong> contributes <span class="text-blue-400">13.93%</span>, continuing to capture unique structure in the dataset.</li>
    <li><strong class="text-white">PC5</strong> explains <span class="text-blue-400">12.82%</span>, still adding meaningful variance.</li>
    <li><strong class="text-white">PC6</strong> covers <span class="text-blue-400">11.80%</span>, bringing the cumulative total to over 92%.</li>
    <li><strong class="text-white">PC7</strong> adds the final <span class="text-blue-400">7.72%</span>, allowing us to retain 100% of the original data‚Äôs variance.</li>
  </ul>

    <table class="w-full table-auto text-center text-gray-300 border-collapse mt-8">
    <thead>
      <tr class="bg-gray-800 text-white border-b border-gray-700">
        <th class="px-6 py-3 text-lg font-semibold">Principal Component</th>
        <th class="px-6 py-3 text-lg font-semibold">Explained Variance Ratio</th>
        <th class="px-6 py-3 text-lg font-semibold">Cumulative Variance</th>
      </tr>
    </thead>
    <tbody>
      <tr class="border-b border-gray-800">
        <td class="py-4">PC1</td>
        <td class="py-4">0.215613</td>
        <td class="py-4">0.215613</td>
      </tr>
      <tr class="border-b border-gray-800">
        <td class="py-4">PC2</td>
        <td class="py-4">0.165002</td>
        <td class="py-4">0.380615</td>
      </tr>
      <tr class="border-b border-gray-800">
        <td class="py-4">PC3</td>
        <td class="py-4">0.156717</td>
        <td class="py-4">0.537332</td>
      </tr>
      <tr class="border-b border-gray-800">
        <td class="py-4">PC4</td>
        <td class="py-4">0.139283</td>
        <td class="py-4">0.676615</td>
      </tr>
      <tr class="border-b border-gray-800">
        <td class="py-4">PC5</td>
        <td class="py-4">0.128202</td>
        <td class="py-4">0.804817</td>
      </tr>
      <tr class="border-b border-gray-800">
        <td class="py-4">PC6</td>
        <td class="py-4">0.118023</td>
        <td class="py-4">0.922840</td>
      </tr>
      <tr>
        <td class="py-4">PC7</td>
        <td class="py-4">0.077160</td>
        <td class="py-4">1.000000</td>
      </tr>
    </tbody>
  </table>

</section>

<div class="max-w-4xl mx-auto my-10">
  <figure class="bg-gray-900 border border-gray-700 rounded-xl shadow-lg p-4">
    <img src="assets/after_pca.png" alt="DATASET AFTER APPLYING PCA" class="rounded-md w-full object-contain">
    <figcaption class="mt-4 text-center text-gray-400 text-sm"><em>DATASET AFTER APPLYING PCA</em></figcaption>
  </figure>
</div>

<div class="flex justify-center mt-8">
  <a href="https://github.com/moukthika-gunapaneedu/Bank-Marketing-Campaign-Analyzer/blob/main/Data/PCA/PCA_transformed_data.csv" 
     target="_blank"
     class="px-8 py-3 bg-blue-600 text-white text-lg font-semibold rounded-lg shadow-lg hover:bg-blue-500 hover:scale-105 transition transform duration-200 ease-in-out">
    üóÉÔ∏è CLICK HERE TO VIEW THE DATASET AFTER APPLYING PCA
  </a>
</div>

<!-- PCA Conclusion Bubble -->
<div class="bg-gray-900 rounded-2xl p-8 shadow-lg border border-gray-800 max-w-6xl mx-auto mt-16">
  <h2 class="text-3xl font-bold text-white mb-6">Conclusion</h2>
  <hr class="border-gray-700 mb-6" />

  <p class="text-gray-300 text-lg leading-relaxed text-justify mb-4">
    The PCA analysis successfully reduced the dataset‚Äôs dimensionality while preserving key information.
    The 2D and 3D projections revealed underlying data patterns, making it easier to identify trends and relationships.
  </p>

  <ul class="list-disc list-inside text-gray-300 text-lg leading-relaxed text-justify mb-4 space-y-2">
    <li>
      The <span class="text-blue-400 font-medium">2D PCA projection</span> retained <span class="text-blue-400 font-semibold">38.06%</span> of the variance,
      providing a simplified yet informative representation of the data.
    </li>
    <li>
      The <span class="text-blue-400 font-medium">3D PCA projection</span> captured <span class="text-blue-400 font-semibold">53.73%</span> of the variance,
      offering a more detailed but still reduced view of the dataset.
    </li>
  </ul>

  <p class="text-gray-300 text-lg leading-relaxed text-justify mb-4">
    Further analysis showed that <span class="text-blue-400 font-semibold">7 principal components</span> were needed to retain at least
    <span class="text-blue-400 font-semibold">95%</span> of the variance, ensuring minimal information loss.
  </p>

  <p class="text-gray-300 text-lg leading-relaxed text-justify">
    The most important principal components were those associated with key financial and engagement attributes,
    highlighting their influence on customer behavior. Overall, PCA played a crucial role in reducing complexity,
    improving visualization, and enhancing further analyses like clustering while retaining the dataset‚Äôs most significant characteristics.
  </p>
</div>

<!-- Footer -->
<footer class="bg-gray-900 py-6 mt-20 text-center border-t border-gray-800">
  <p class="text-gray-400 text-sm">
    ¬© 2025 Moukthika | 
    <a href="https://github.com/moukthika-gunapaneedu" target="_blank" class="underline hover:text-blue-400">
      GitHub
    </a>
  </p>

</body>
</html>
